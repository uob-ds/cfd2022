{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f49728",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01538736",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Safe settings for Pandas.\n",
    "pd.set_option('mode.chained_assignment', 'raise')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Make the plots look more fancy.\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Optimization function\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80edf4b4",
   "metadata": {},
   "source": [
    "By Peter Rush and Matthew Brett, with considerable inspiration from the\n",
    "logistic regression section of Allen Downey's book [Think Stats, second\n",
    "edition](https://greenteapress.com/thinkstats2).\n",
    "\n",
    "In this section we will look at another regression technique: logistic\n",
    "regression.\n",
    "\n",
    "We use logistic regression when we want to predict a *binary categorical*\n",
    "outcome variable (or column) from one or more predicting variables (or\n",
    "columns).\n",
    "\n",
    "A binary categorical variable is one where an observation can fall into one of\n",
    "only two categories. We give each observation a label corresponding to their\n",
    "category.  Some examples are:\n",
    "\n",
    "* Did a patient die or did they survive through 6 months of treatment?  The\n",
    "  patient can only be in only one of the categories.  In some column of our\n",
    "  data table, patients that died might have the label \"died\", and those who\n",
    "  have survived have the label \"survived\".\n",
    "* Did a person experience more than one episode of psychosis in the last 5\n",
    "  years (\"yes\" or \"no\")?\n",
    "* Did a person with a conviction for one offense offend again (\"yes\" or \"no\")?\n",
    "\n",
    "For this tutorial, we return to the [chronic kidney disease\n",
    "dataset](../data/chronic_kidney_disease).\n",
    "\n",
    "Each row in this dataset represents one patient.\n",
    "\n",
    "For each patient, the doctors recorded whether or not the patient had chronic\n",
    "kidney disease. This is a *binary categorical variable*; you can see the\n",
    "values in the \"Class\" column. A value of 1 means the patient *did* have CKD; a\n",
    "value of 0 means they *did not*.  In this case we are labeling the categories\n",
    "with numbers (1 / 0).\n",
    "\n",
    "Many of the rest of the columns are measurements from blood tests and urine tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feddec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ckd_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976235e2",
   "metadata": {},
   "source": [
    "There are actually a large number of binary categorical variables in this\n",
    "dataset.   For example, the \"Hypertension\" column has labels for the two\n",
    "categories \"yes\" (the patient had persistently high blood pressure) or \"no\".\n",
    "\n",
    "The categorical variable we are interested in here is \"Appetite\".  This has\n",
    "the label \"good\" for patients with good appetite, and \"poor\" for those with\n",
    "poor appetite.  Poor appetite is a [symptom of chronic kidney\n",
    "disease](https://www.sciencedirect.com/science/article/abs/pii/S0270929508001666).  In our case, we wonder whether the extent of kidney damage does a convincing job in predicting whether the patient has a \"good\" appetite.\n",
    "\n",
    "As you remember, the CKD dataset has a column \"Hemoglobin\" that has the\n",
    "concentration of hemoglobin from a blood sample.  Hemoglobin is the molecule\n",
    "that carries oxygen in the blood; it is the molecule that makes the red blood\n",
    "cells red.  Damaged kidneys produce lower concentrations of the hormone that\n",
    "stimulates red blood cell production,\n",
    "[erythropoietin](https://en.wikipedia.org/wiki/Erythropoietin), so CKD\n",
    "patients often have fewer red blood cells, and lower concentrations of\n",
    "Hemoglobin.  We will take lower \"Hemoglobin\" as a index of kidney damage.\n",
    "Therefore, we predict that patients with lower \"Hemoglobin\" values are more\n",
    "likely to have `poor` \"Appetite\" values, and, conversely, patients with higher\n",
    "\"Hemoglobin\" values are more likely to have `good` \"Appetite\" values.\n",
    "\n",
    "First we make a new data frame that just has the two columns we are interested\n",
    "in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986669ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_app = df.loc[:, ['Hemoglobin', 'Appetite']].copy()\n",
    "hgb_app.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc2deb",
   "metadata": {},
   "source": [
    "## Dummy Variables\n",
    "\n",
    "We will soon find ourselves wanting to do calculations on the values in the\n",
    "\"Appetite\" column, and we cannot easily do that with the current string values\n",
    "of \"good\" and \"poor\".   Our next step is to recode the string values to\n",
    "numbers, ready for our calculations.  We use 1 to represent \"good\" and 0 to\n",
    "represent \"poor\".  This kind of recoding, where we replace category labels\n",
    "with 1 and 0 values, is often called *dummy coding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'replace' replaces the values in the first argument with the corresponding\n",
    "# values in the second argument.\n",
    "hgb_app['appetite_dummy'] = hgb_app['Appetite'].replace(\n",
    "    ['poor', 'good'],\n",
    "    [0, 1])\n",
    "hgb_app.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b41f1",
   "metadata": {},
   "source": [
    "*Note*: When you are doing this, be sure to keep track of which label you have\n",
    "coded as 1. Normally this would be the more interesting outcome.  In this\n",
    "case, \"good\" has the code 1. Keep track of the label corresponding to 1, as it\n",
    "will affect the interpretation of the regression coefficients.\n",
    "\n",
    "Now we have the dummy (1 or 0) variable, let us use a scatter plot to look at\n",
    "the relationship between hemoglobin concentration and whether a patient has a\n",
    "good appetite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198fcad5",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# We will use this plotting code several times, so put into a function for\n",
    "# later use.\n",
    "\n",
    "def plot_hgb_app():\n",
    "    # Build plot, add custom labels.\n",
    "    colors = hgb_app['Appetite'].replace(['poor', 'good'], ['red', 'blue'])\n",
    "    hgb_app.plot.scatter('Hemoglobin', 'appetite_dummy', c=colors)\n",
    "    plt.ylabel('Appetite\\n0 = poor, 1 = good')\n",
    "    plt.yticks([0,1]);  # Just label 0 and 1 on the y axis.\n",
    "    # Put a custom legend on the plot.  This code is a little obscure.\n",
    "    plt.scatter([], [], c='blue', label='good')\n",
    "    plt.scatter([], [], c='red', label='poor')\n",
    "\n",
    "# Do the plot\n",
    "plot_hgb_app()\n",
    "# Show the legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14c014",
   "metadata": {},
   "source": [
    "From the plot, it does look as if the patients with lower hemoglobin are more\n",
    "likely to have poor appetite (`appetite_dummy` values of 0), whereas patients\n",
    "with higher hemoglobin tend to have good appetite (`appetite_dummy` values of\n",
    "1).\n",
    "\n",
    "Now we start to get more formal, and develop a model with which we predict the\n",
    "`appetite_dummy` values from the  `Hemoglobin` values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ac249",
   "metadata": {},
   "source": [
    "## How about linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34071382",
   "metadata": {},
   "source": [
    "Remember that, in linear regression, we predict scores on the *outcome*\n",
    "variable (or column) using a straight-line relationship of the *predictor*\n",
    "variable (or column).\n",
    "\n",
    "Here are the predictor and outcome variables in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The x (predictor) and y (outcome) variables.\n",
    "hemoglobin = hgb_app['Hemoglobin']\n",
    "appetite_d = hgb_app['appetite_dummy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08ef66",
   "metadata": {},
   "source": [
    "Why not use the same linear regression technique for our case?  After all, the\n",
    "`appetite_d` values are just numbers (0 and 1), as are our `hemoglobin` values.\n",
    "\n",
    "Earlier in the textbook, we performed linear regression by using `minimize`, to\n",
    "find the value of the slope and intercept of the line which gives the smallest\n",
    "sum of the squared prediction errors.\n",
    "\n",
    "Recall that, in linear regression:\n",
    "\n",
    "$$\n",
    "\\text{predicted} = intercept + slope * \\text{predictor_variable}\n",
    "$$\n",
    "\n",
    "*predicted* and *predictor variable* here are sequences of values, with one\n",
    "value for each observation (row) in the dataset. In our case we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows in CKD data', len(hgb_app))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fd53c",
   "metadata": {},
   "source": [
    "\"observations\" (patients), so there will be the same number of scores on the\n",
    "predictor variable (`hemoglobin`), and the same number of predictions, in\n",
    "`predicted`. By contrast, the slope and intercept are single values, defining\n",
    "the line.\n",
    "\n",
    "We used `minimize` to find the values of the slope and intercept which give the\n",
    "\"best\" predictions.  So far, we have almost invariably defined the *best*\n",
    "values for slope and intercept as the values that give the smallest sum of the\n",
    "squared prediction errors.\n",
    "\n",
    "$$\n",
    "\\text{prediction errors} = \\text{actual variable - predicted}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb728904",
   "metadata": {},
   "source": [
    "What would happen if we tried to use linear regression to predict the\n",
    "probability of having good appetite, based on hemoglobin concentrations?\n",
    "\n",
    "Let us start by grabbing a version of the `ss_any_line` function from the\n",
    "[Using minimize page](../mean-slopes/using_minimize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc757d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_any_line(c_s, x_values, y_values):\n",
    "    # c_s is a list containing two elements, an intercept and a slope.\n",
    "    intercept, slope = c_s\n",
    "    # Values predicted from these x_values, using this intercept and slope.\n",
    "    predicted = intercept + x_values * slope\n",
    "    # Difference of prediction from the actual y values.\n",
    "    error = y_values - predicted\n",
    "    # Sum of squared error.\n",
    "    return np.sum(error ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4f846",
   "metadata": {},
   "source": [
    "The sum of squared prediction error, in linear regression, is our *cost*\n",
    "function. When we have a good pair of (intercept, slope) in `c_s`, our function\n",
    "is *cheap* - i.e. the returned value is small.  When we have a bad pair in\n",
    "`c_s`, our function is *expensive* - the returned value is large.\n",
    "\n",
    "If the value from `ss_any_line` is large, it means the line we are fitting does\n",
    "not fit the data well. The purpose of linear regression is to find the\n",
    "line which leads to the smallest cost.  In our case, the cost is the sum of the\n",
    "squared prediction errors.\n",
    "\n",
    "Let's use linear regression on the current example.  From looking at our plot above, we start with a guess of -1 for the intercept, and 0.1 for the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ededc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use minimize to find the least sum of squares solution.\n",
    "min_lin_reg = minimize(ss_any_line, [-1, 0.1], args=(hemoglobin, appetite_d))\n",
    "# Show the results that came back from minimize.\n",
    "min_lin_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb860b",
   "metadata": {},
   "source": [
    "OK, so that looks hopeful. Using linear regression with `minimize` we found that the sum of squared prediction errors was smallest for a line with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the slope and intercept estimates from the results object.\n",
    "lin_reg_intercept, lin_reg_slope = min_lin_reg.x\n",
    "# Show them.\n",
    "print('Best linear regression intercept', lin_reg_intercept)\n",
    "print('Best linear regression slope', lin_reg_slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39dd8eb",
   "metadata": {},
   "source": [
    "The linear regression model we are using here is:\n",
    "\n",
    "```\n",
    "predicted_appetite_d = intercept + slope * hemoglobin\n",
    "```\n",
    "\n",
    "Specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lin_reg = lin_reg_intercept + lin_reg_slope * hemoglobin\n",
    "predicted_lin_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5fdd8",
   "metadata": {},
   "source": [
    "Let's plot our predictions, alongside the actual data.  We plot the predictions\n",
    "from linear regression in orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c492e0",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Do the base plot of the hemoglobin and appetite_d.\n",
    "plot_hgb_app()\n",
    "\n",
    "# A new plot on top of the old.\n",
    "plt.scatter(hemoglobin, predicted_lin_reg,\n",
    "            label='LR prediction',\n",
    "            color='orange')\n",
    "# Another plot, to show the underlying line\n",
    "fine_x = np.linspace(np.min(hemoglobin), np.max(hemoglobin), 1000)\n",
    "fine_y = lin_reg_intercept + lin_reg_slope * fine_x\n",
    "plt.plot(fine_x, fine_y, linewidth=1, linestyle=':')\n",
    "# Show the legend.\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56bea6",
   "metadata": {},
   "source": [
    "The linear regression line looks plausible, as far as it goes, but it has\n",
    "several unhappy features for our task of predicting the `appetite_d` 0 / 1\n",
    "values.\n",
    "\n",
    "One thing to like about the line is that the predictions are right to suggest\n",
    "that the value of `appetite_d` is more likely to be 1 (meaning \"good\") at\n",
    "higher values of `hemoglobin`.  Also, the prediction line slopes upward as\n",
    "`hemoglobin` gets higher, indicating that the probability of good appetite gets\n",
    "higher as the hemoglobin concentration rises, across patients.\n",
    "\n",
    "However, when the `hemoglobin` gets higher than about 15.5, linear regression\n",
    "starts to predict a value for `appetite_d` that is greater than 1 - which, of\n",
    "course, cannot occur in the `appetite_d` values, which are restricted to 0 or\n",
    "1.\n",
    "\n",
    "Looking at the plot, without the regression line, it looks as if we can be\n",
    "fairly confident of predicting a 1 (\"good\") value for a hemoglobin above 12.5, but we are increasingly less confident about predicting a 1 value as hemoglobin drops down to about 7.5, at which point we become confident about predicting a 0 value.\n",
    "\n",
    "These reflections make as wonder whether we should be using something other\n",
    "than a simple, unconstrained straight line for our predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de323209",
   "metadata": {},
   "source": [
    "## Another prediction line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe51a0",
   "metadata": {},
   "source": [
    "Here's another prediction line we might use for `appetite_d`, with the\n",
    "predicted values.\n",
    "\n",
    "For the moment, please don't worry about how we came by this line, we will come\n",
    "onto that soon.\n",
    "\n",
    "The new prediction line is in gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573b45f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the machinery for making the sigmoid line of the plots below.  We\n",
    "# will come on that machinery soon.  For now please ignore this code, and\n",
    "# concentrate on the plots below.\n",
    "\n",
    "def inv_logit(y):\n",
    "    \"\"\" Reverse logit transformation\n",
    "    \"\"\"\n",
    "    odds_ratios = np.exp(y)  # Reverse the log operation.\n",
    "    return odds_ratios / (odds_ratios + 1)  # Reverse odds ratios operation.\n",
    "\n",
    "\n",
    "def params2pps(intercept, slope, x):\n",
    "    \"\"\" Calculate predicted probabilities of 1 for each observation.\n",
    "    \"\"\"\n",
    "    # Predicted log odds of being in class 1.\n",
    "    predicted_log_odds = intercept + slope * x\n",
    "    return inv_logit(predicted_log_odds)\n",
    "\n",
    "\n",
    "# Some plausible values for intercept and slope.\n",
    "nice_intercept, nice_slope = -7, 0.8\n",
    "predictions_new = params2pps(nice_intercept, nice_slope, hemoglobin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f312a13",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Do the base plot of the hemoglobin and appetite_d.\n",
    "plot_hgb_app()\n",
    "\n",
    "# A new plot on top of the old.\n",
    "plt.scatter(hemoglobin, predictions_new,\n",
    "            label='New prediction',\n",
    "            color='gold')\n",
    "# Another plot, to show the underlying line\n",
    "fine_y_sigmoid = params2pps(nice_intercept, nice_slope, fine_x)\n",
    "plt.plot(fine_x, fine_y_sigmoid, linewidth=1, linestyle=':')\n",
    "# Show the legend.\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2b18a",
   "metadata": {},
   "source": [
    "The new not-straight line seems to have much to recommend it.  This shape of\n",
    "line is called \"sigmoid\", from the name of the Greek letter \"s\".  The sigmoid\n",
    "prediction here never goes above 1 or below 0, so its values are always in the\n",
    "range of the `appetite_d` data it is trying to predict.  It climbs steeply to a\n",
    "prediction of 1, and plateaus there, as we get to the threshold of hemoglobin\n",
    "around 12.5, at which every patient does seem to have \"good\" appetite\n",
    "(`appetite_d` of 1).\n",
    "\n",
    "We can think of the values from the sigmoid curve as being *predicted\n",
    "probabilities*.  For example, at a `hemoglobin` value of 10, the curve gives a\n",
    "predicted y (`appetite_d`) value of about 0.73.  We can interpret this\n",
    "prediction as saying that, with a hemoglobin value of 10, there is a\n",
    "*probability* of about 0.73 that the corresponding patient will have a \"good\"\n",
    "appetite (`appetite_d` value of 1).\n",
    "\n",
    "Now let us say that we would prefer to use this kind of sigmoid line to predict\n",
    "`appetite_d`.  So far, we have only asked `minimize` to predict directly from a\n",
    "straight line - for example, in the `ss_any_line` function.   How can we get\n",
    "minimize to predict from a family of sigmoid curves, as here?  Is there a way\n",
    "of transforming a sigmoid curve like the one here, with y values from 0 to 1,\n",
    "to a straight line, where the y values can vary from large negative to large\n",
    "positive?  We would like to do such a conversion, so we have a slope and\n",
    "intercept that `minimize` can work with easily.\n",
    "\n",
    "The answer, you can imagine, is \"yes\" — we can go from our sigmoid 0 / 1 curve\n",
    "to a straight line with unconstrained y values, in two fairly simple steps. The\n",
    "next sections will cover those steps.  The two steps are:\n",
    "\n",
    "* Convert the 0 / 1 *probability* predictions to 0-to-large positive\n",
    "  predictions of the *odds ratio*.  The odds-ratio can vary from 0 to very\n",
    "  large positive.\n",
    "* Apply the *logarithm* function to convert the 0-to-very-large-positive\n",
    "  odds-ratio predictions to log odds-ratio predictions, which can vary from\n",
    "  very large negative to very large positive.\n",
    "\n",
    "These two transformations together are called the *log-odds* or\n",
    "[logit](https://en.wikipedia.org/wiki/Logit) transformation.  *Logistical\n",
    "regression* is regression using the *logit* transform.  Applying the logit\n",
    "transform converts the sigmoid curve to a straight line.\n",
    "\n",
    "We will explain more about the two stages of the transform below, but for now, here are the two stages in action.\n",
    "\n",
    "This is the original sigmoid curve above, with the predictions, in its own\n",
    "plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44667952",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(hemoglobin, predictions_new, color='gold')\n",
    "plt.plot(fine_x, fine_y_sigmoid, linewidth=1, linestyle=':')\n",
    "plt.title('Sigmoid probability prediction')\n",
    "plt.xlabel('Hemoglobin')\n",
    "plt.ylabel('Probability prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f83e49",
   "metadata": {},
   "source": [
    "Next we apply the conversion from probability to odds-ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a34a8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "predictions_or = predictions_new / (1 - predictions_new)\n",
    "plt.scatter(hemoglobin, predictions_or, color='gold')\n",
    "fine_y_or = fine_y_sigmoid / (1 - fine_y_sigmoid)\n",
    "plt.plot(fine_x, fine_y_or, linewidth=1, linestyle=':')\n",
    "plt.title('Odds-ratio prediction')\n",
    "plt.xlabel('Hemoglobin')\n",
    "plt.ylabel('Odds-ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615dc77",
   "metadata": {},
   "source": [
    "Notice that this is an *exponential* graph, where the y values increase more\n",
    "and more steeply as the x values increase.  We can turn exponential lines like\n",
    "this one into straight lines, using the *logarithm* function, the next stage of\n",
    "the logit transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_or_log = np.log(predictions_or)\n",
    "plt.scatter(hemoglobin, predictions_or_log, color='gold')\n",
    "fine_y_or_log = np.log(fine_y_or)\n",
    "plt.plot(fine_x, fine_y_or_log, linewidth=1, linestyle=':')\n",
    "plt.title('Logit prediction')\n",
    "plt.xlabel('Hemoglobin')\n",
    "plt.ylabel('Log odds-ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f498f",
   "metadata": {},
   "source": [
    "Now we have a straight line, with an intercept and slope, suitable for\n",
    "`minimize`.  The next few sections go into more detail on the odds-ratio and\n",
    "logarithm steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386030f9",
   "metadata": {},
   "source": [
    "## Probability and Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde310e3",
   "metadata": {},
   "source": [
    "For logistic regression, in contrast to linear regression, we are interested in\n",
    "predicting the *probability of an observation falling into a particular outcome\n",
    "class* (0 or 1).\n",
    "\n",
    "In this case, we are interested in the probability of a patient having \"good\"\n",
    "appetite, predicted from the patient's hemoglobin.\n",
    "\n",
    "We can think of probability as the *proportion of times* we expect to see a\n",
    "particular outcome.\n",
    "\n",
    "For example, there are 139 patients with \"good\" appetite in this data frame,\n",
    "and 158 patients in total.  If you were to repeatedly draw a single patient at\n",
    "random from the data frame, and record their `Appetite`, then we expect the proportion of \"good\" values in the long run, to be 139 / 158 --- about 0.88.  That is the same as saying there is a probability of 0.88 of a randomly-drawn patient of having a \"good\" appetite.\n",
    "\n",
    "Because the patient's appetite can only be \"good\" or \"poor\", and because the\n",
    "probabilities of all possible options have to add up to 1, the probability of\n",
    "the patient having a \"poor\" appetite is 1 - 0.88 --- about 0.12.\n",
    "\n",
    "So, the probability can express the *proportion of times* we expect to see some\n",
    "*event of interest* - in our case, the event of interest is \"good\" in the\n",
    "`Appetite` column.\n",
    "\n",
    "We can think of this same information as an *odds ratio*.\n",
    "\n",
    "We often express probabilities as odds ratios.  For example, we might say that\n",
    "the odds are 5 to 1 that it will rain today.   We mean that it is five times\n",
    "more likely that it will rain today than that it will not rain today.   On\n",
    "another day we could estimate that the odds are 0.5 to 1 that it will rain\n",
    "today, meaning that it is twice as likely *not* to rain, as it is to rain.\n",
    "\n",
    "The odds ratio is the number of times we expect to see the event of interest\n",
    "(e.g. rain) for every time we expect to see the event of no interest (not\n",
    "rain).\n",
    "\n",
    "This is just a way of saying a probability in a different way, and we can convert easily between probabilities and odds ratios.\n",
    "\n",
    "To convert from a probability to an odds ratio, we remember that the odds ratio is the number of times we expect to see the event of interest for every time we expect to see the event of no interest.   This is the probability (proportion) for the event of interest, divided by the probability (proportion) of the event of no interest.  Say the probability p is some value (it could be any value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e980de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our proportion of interest.\n",
    "p = 0.88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb2a62",
   "metadata": {},
   "source": [
    "Then the equivalent odds ratio `odds_ratio` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb96d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds ratio is proportion of interest, divided by proportion of no interest.\n",
    "odds_ratio = p / (1 - p)\n",
    "odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104dc7c0",
   "metadata": {},
   "source": [
    "Notice the odds ratio is greater than one, because `p` was greater than 0.5. An\n",
    "odds ratio of greater than one means the event of interest is more likely to\n",
    "happen than the alternative.  An odds ratio of less than one means p was less\n",
    "than 0.5, and the event of interest is less likely to happen than the\n",
    "alternative.   A p value of 0 gives an odds ratio of 0.  As the p value gets close to 1, the odds ratio gets very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.999999\n",
    "p / (1 - p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5df1fe",
   "metadata": {},
   "source": [
    "We can also convert from odds ratios to p values.  Remember the odds ratio is\n",
    "the number of times we expect to see the event of interest, divided by the\n",
    "number of times we expect to see the event of no interest.  The probability is\n",
    "the proportion of all events that are events of interest.  We can read an\n",
    "`odds_ratio` of - say - 2.5 as \"the chances are 2.5 to 1\".  To get the\n",
    "probability we divide the number of times we expect to see the event of\n",
    "interest - here 2.5 - by the number of events in total.  The number of events\n",
    "in total is just the number of events of interest - here 2.5 - plus the number\n",
    "of events of no interest - here 1.  So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_from_odds_ratio = odds_ratio / (odds_ratio + 1)\n",
    "p_from_odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b240aa",
   "metadata": {},
   "source": [
    "Summary - convert probabilities to odds ratios with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc61158",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio = p / (1 - p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ca9c4",
   "metadata": {},
   "source": [
    "Convert odds ratios to probabilities with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = odds_ratio / (odds_ratio + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054da58",
   "metadata": {},
   "source": [
    "As you've seen, when we apply the conversion above, to convert the probability\n",
    "values to odds ratios for our appetite predictions, we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af2286",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "predictions_or = predictions_new / (1 - predictions_new)\n",
    "plt.scatter(hemoglobin, predictions_or,\n",
    "            color='gold')\n",
    "fine_y_or = fine_y_sigmoid / (1 - fine_y_sigmoid)\n",
    "plt.plot(fine_x, fine_y_or, linewidth=1, linestyle=':')\n",
    "plt.title('Odds-ratio prediction')\n",
    "plt.xlabel('Hemoglobin')\n",
    "plt.ylabel('Odds-ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa6564",
   "metadata": {},
   "source": [
    "Notice that the odds ratios we found vary from very close to 0 (for p value\n",
    "predictions very close to 0) to very large (for p values very close to 1).\n",
    "\n",
    "Notice too that our graph looks exponential, and we want it to be a straight\n",
    "line.   Our next step is to apply a *logarithm* transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424254d",
   "metadata": {},
   "source": [
    "## The logarithm transform\n",
    "\n",
    "See the [logarithm refresher](logarithms_refreshed) page for more background on\n",
    "logarithms.\n",
    "\n",
    "For now, the only thing you need to know about logarithms is that they are\n",
    "transformations that convert an exponential curve into a straight line.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59844971",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1000)\n",
    "y = 3 ** x\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "ax0, ax1 = axes\n",
    "ax0.plot(x, y)\n",
    "ax0.set_title(r'$y=3^x$')\n",
    "ax1.plot(x, np.log(y))\n",
    "ax1.set_title(r'Log of $3^x$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e8ae8",
   "metadata": {},
   "source": [
    "You have already see logs in action transforming the odds-ratio predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e89fe",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "ax0, ax1 = axes\n",
    "ax0.scatter(hemoglobin, predictions_or, color='gold')\n",
    "ax0.plot(fine_x, fine_y_or, linewidth=1, linestyle=':')\n",
    "ax0.set_title('Odds-ratio')\n",
    "ax0.set_xlabel('Hemoglobin')\n",
    "ax0.set_ylabel('Odds-ratio');\n",
    "ax1.scatter(hemoglobin, predictions_or_log, color='gold')\n",
    "ax1.plot(fine_x, fine_y_or_log, linewidth=1, linestyle=':')\n",
    "ax1.set_title('Log odds-ratio')\n",
    "ax1.set_xlabel('Hemoglobin')\n",
    "ax1.set_ylabel('Log odds-ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b02c5a",
   "metadata": {},
   "source": [
    "## The logit transform and its inverse\n",
    "\n",
    "The logit transformation from the sigmoid curve to the straight line consists\n",
    "of two steps:\n",
    "\n",
    "* Convert probability to odd-ratios.\n",
    "* Take the log of the result.\n",
    "\n",
    "The full logit transformation is therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945153d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    \"\"\" Apply logit transformation to array of probabilities `p`\n",
    "    \"\"\"\n",
    "    odds_ratios = p / (1 - p)\n",
    "    return np.log(odds_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6888dff",
   "metadata": {},
   "source": [
    "Here are the original sigmoid predictions and the predictions with the `logit` transform applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128f8f1",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "ax0, ax1 = axes\n",
    "ax0.scatter(hemoglobin, predictions_new, color='gold')\n",
    "ax0.plot(fine_x, fine_y_sigmoid, linewidth=1, linestyle=':')\n",
    "ax0.set_title('Sigmoid p')\n",
    "ax0.set_xlabel('Hemoglobin')\n",
    "ax0.set_ylabel('Sigmoid p values');\n",
    "ax1.scatter(hemoglobin, logit(predictions_new), color='gold')\n",
    "ax1.plot(fine_x, logit(fine_y_sigmoid), linewidth=1, linestyle=':')\n",
    "ax1.set_title('Logit on sigmoid')\n",
    "ax1.set_xlabel('Hemoglobin')\n",
    "ax1.set_ylabel('Log odds-ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6710be",
   "metadata": {},
   "source": [
    "We also want to be able to go backwards, from the straight-line predictions, to\n",
    "the sigmoid predictions.\n",
    "\n",
    "`np.exp` reverses (inverts) the `np.log` transformation (see the [logarithm\n",
    "refresher](logarithms_refreshed) page):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.exp reverses the effect of np.log.\n",
    "some_values = np.array([1, 0.5, 3, 6, 0.1])\n",
    "values_back = np.exp(np.log(some_values))\n",
    "values_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0febe6",
   "metadata": {},
   "source": [
    "You have seen above that there is a simple formula to go from odds ratios to\n",
    "probabilities.  The transformation that *reverses*  (inverts) the logit\n",
    "transform is therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(v):\n",
    "    \"\"\" Reverse logit transformation on array `v`\n",
    "    \"\"\"\n",
    "    odds_ratios = np.exp(v)  # Reverse the log operation.\n",
    "    return odds_ratios / (odds_ratios + 1)  # Reverse odds ratios operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90140e5c",
   "metadata": {},
   "source": [
    "`inv_logit` takes points on a straight line, and converts them to points on a\n",
    "sigmoid.\n",
    "\n",
    "First we convince ourselves that `inv_logit` does indeed reverse the `logit`\n",
    "transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9633d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_p_values = np.array([0.01, 0.05, 0.1, 0.5, 0.9, 0.95, 0.99])\n",
    "some_log_odds = logit(some_p_values)\n",
    "back_again = inv_logit(some_log_odds)\n",
    "print('Logit, then inv_logit returns the original data', back_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78268d4e",
   "metadata": {},
   "source": [
    "The plot above has the sigmoid curve p-value predictions, and the p-value\n",
    "predictions with the logit transformation applied.\n",
    "\n",
    "Next you see the logit-transformed results on the left.  The right shows the\n",
    "results of applying `inv_logit` to recover the original p values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6a14e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "ax0, ax1 = axes\n",
    "ax0.scatter(hemoglobin, predictions_or_log, color='gold')\n",
    "ax0.set_title('Logit values')\n",
    "ax0.set_xlabel('Hemoglobin')\n",
    "ax0.set_ylabel('Logit transformed');\n",
    "ax0.plot(fine_x, fine_y_or_log, linewidth=1, linestyle=':')\n",
    "ax1.scatter(hemoglobin, predictions_new,\n",
    "            color='gold')\n",
    "ax1.set_title('inv_logit result')\n",
    "ax1.set_xlabel('Hemoglobin')\n",
    "ax1.set_ylabel('inv_logit result');\n",
    "ax1.plot(fine_x, inv_logit(fine_y_or_log), linewidth=1, linestyle=':');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca4373",
   "metadata": {},
   "source": [
    "## Effect of the Logit slope and intercept on the sigmoid\n",
    "\n",
    "Changing the intercept of the logit (log-odds) straight line moves the\n",
    "corresponding inverse logit sigmoid curve left and right on the horizontal\n",
    "axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intercept in [-6, -7, -8]:\n",
    "    plt.plot(fine_x,\n",
    "            params2pps(intercept, 0.8, fine_x),\n",
    "            linewidth=1,\n",
    "            linestyle=':',\n",
    "            label='logit intercept=%d' % intercept)\n",
    "plt.title('Sigmoid probability predictions')\n",
    "plt.xlabel('Hemoglobin')\n",
    "plt.ylabel('Probability prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b747c",
   "metadata": {},
   "source": [
    "Changing the slope of the logit straight line makes the transition from 0 to 1\n",
    "flatter or steeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2fdad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for slope in [0.6, 0.8, 1.0]:\n",
    "    plt.plot(fine_x,\n",
    "            params2pps(-7, slope, fine_x),\n",
    "            linewidth=1,\n",
    "            linestyle=':',\n",
    "            label='logit slope=%.1f' % slope)\n",
    "plt.title('Sigmoid probability predictions')\n",
    "plt.xlabel('Hemoglobin')\n",
    "plt.ylabel('Probability prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef680b",
   "metadata": {},
   "source": [
    "## A first-pass at logistic regression\n",
    "\n",
    "You may now see how we could use `minimize` with a slope and intercept.\n",
    "\n",
    "Remember that `minimize` needs a *cost function* that takes some parameters -\n",
    "in our case, the intercept and slope — and returns a score, or cost for the\n",
    "parameters.\n",
    "\n",
    "In the `ss_any_line` cost function above, the score it returns is just the sum\n",
    "of squared prediction errors from the line.  But the cost function can do\n",
    "anything it likes to generate a score from the line.\n",
    "\n",
    "In our case, we're going to make a cost function that takes the intercept and\n",
    "slope, and generates predictions using the intercept and slope and `hemoglobin`\n",
    "values.  But these predictions are for the log-odds transformed values, on the\n",
    "straight line.  So the cost function then converts the predictions into p value\n",
    "predictions, on the corresponding sigmoid, and compares these predictions to the 0 / 1 values in `appetite_d`.\n",
    "\n",
    "For example, let's say we want to get a score for the intercept -7 and the\n",
    "slope 0.8.\n",
    "\n",
    "First we get the straight-line predictions in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept, slope = -7, 0.8\n",
    "sl_predictions = intercept + slope * hemoglobin\n",
    "\n",
    "plt.scatter(hemoglobin, sl_predictions)\n",
    "plt.xlabel('Hemoglobin')\n",
    "plt.ylabel('Straight-line (log-odds) predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54f589",
   "metadata": {},
   "source": [
    "These are predictions on the straight line, but we now need to transform them\n",
    "to p value (0 to 1) predictions on the sigmoid.  We use `inv_logit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_predictions = inv_logit(sl_predictions)\n",
    "\n",
    "plt.scatter(hemoglobin, sigmoid_predictions)\n",
    "plt.xlabel('Hemoglobin')\n",
    "plt.ylabel('Sigmoid (p-value) predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34579c",
   "metadata": {},
   "source": [
    "Finally, we want to compare the predictions to the actual data to get a score.\n",
    "One way we could do this is our good old sum of squares difference between the\n",
    "sigmoid p-value predictions and the 0 / 1 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_error = appetite_d - sigmoid_predictions\n",
    "sigmoid_score = np.sum(sigmoid_error ** 2)\n",
    "sigmoid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d58d7",
   "metadata": {},
   "source": [
    "Next we make the cost function that minimize will use.  It must accept an array\n",
    "of parameters (an intercept and slope), and calculate the sum of squares error,\n",
    "using the predicted p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_logit(c_s, x_values, y_values):\n",
    "    # Unpack intercept and slope into values.\n",
    "    intercept, slope = c_s\n",
    "    # Predicted values for log-odds straight line.\n",
    "    predicted_log_odds = intercept + slope * x_values\n",
    "    # Predicted p values on sigmoid.\n",
    "    pps = inv_logit(predicted_log_odds)\n",
    "    # Prediction errors.\n",
    "    sigmoid_error = y_values - pps\n",
    "    # Sum of squared error\n",
    "    return np.sum(sigmoid_error ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d9df7",
   "metadata": {},
   "source": [
    "We check our function gives the same results as the step-by-step calculation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_logit([-7, 0.8], hemoglobin, appetite_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db78772",
   "metadata": {},
   "source": [
    "Notice what is happening here.  The cost function gets the new intercept and\n",
    "slope to try, makes the predictions from the intercept and slope, converts the\n",
    "predictions to probabilities on the sigmoid, and tests those against the real\n",
    "labels.\n",
    "\n",
    "Now let's see the cost function in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c352a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_res_logit = minimize(ss_logit, [-7, 0.8], args=(hemoglobin, appetite_d))\n",
    "min_res_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f0dda",
   "metadata": {},
   "source": [
    "Does this result look like it gives more convincing sigmoid predictions than\n",
    "our guessed intercept an slope of -7 and 0.8?\n",
    "\n",
    "First get the sigmoid predictions from this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_ss_inter, logit_ss_slope = min_res_logit.x\n",
    "# Predicted values on log-odds straight line.\n",
    "predicted_log_odds = logit_ss_inter + logit_ss_slope * hemoglobin\n",
    "# Predicted p values on sigmoid.\n",
    "logit_ss_pps = inv_logit(predicted_log_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1252f24",
   "metadata": {},
   "source": [
    "Then plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75947421",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_hgb_app()\n",
    "# A new plot on top of the old.\n",
    "plt.scatter(hemoglobin, logit_ss_pps,\n",
    "            label='Logit ss solution',\n",
    "            color='gold')\n",
    "# Show the legend.\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d7793",
   "metadata": {},
   "source": [
    "## A different measure of prediction error\n",
    "\n",
    "Our sigmoid prediction from sum of squares above looks convincing enough, but,\n",
    "is there a better way of scoring the predictions from our line, than sum of\n",
    "squares?\n",
    "\n",
    "It turns out there is another quite different and useful way to score the\n",
    "predictions, called *likelihood*.  For reasons we discuss in [this\n",
    "page](logistic_convexity), all standard implementations of logistic regression\n",
    "that we know of, use the *likelihood* measure that we describe below, instead\n",
    "of the sum of squares measure you see above.\n",
    "\n",
    "Likelihood asks the question: assuming our predicting line, how likely are the\n",
    "sequence of actual 0 / 1 values that we see?\n",
    "\n",
    "To answer this question, we first ask this question about the individual 0 / 1\n",
    "values.\n",
    "\n",
    "We start with our intercept of -7 and slope of 0.8 for the straight-line\n",
    "log-odds values.  We generate the straight-line predictions, then convert them\n",
    "to sigmoid p-value predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbd2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds_predictions = -7 + hemoglobin * 0.8\n",
    "sigmoid_p_predictions = inv_logit(log_odds_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7008d",
   "metadata": {},
   "source": [
    "Remember, these are the predicted probabilities of a 1 label.  We rename to remind ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff102ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_of_1 = sigmoid_p_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d75d21",
   "metadata": {},
   "source": [
    "We put these predictions into a copy of our data set to make them easier to display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_predicted = hgb_app.copy()\n",
    "hgb_predicted['pp_of_1'] = pp_of_1\n",
    "hgb_predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8e801",
   "metadata": {},
   "source": [
    "Consider the last row in this data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row = hgb_predicted.tail(1)\n",
    "last_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05ba0c",
   "metadata": {},
   "source": [
    "The `pp_of_1` value is the probability that we will see a label of 1 for this\n",
    "observation.   The outcome was, in fact, 1.  Therefore the predicted\n",
    "probability that we will see the actual value is just the corresponding\n",
    "`pp_of_1` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_of_last_value = pp_of_1.iloc[-1]\n",
    "pp_of_last_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e3c50",
   "metadata": {},
   "source": [
    "Now consider the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = hgb_predicted.head(1)\n",
    "first_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2956c",
   "metadata": {},
   "source": [
    "The probability for an `appetite_dummy` value of 1 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_of_1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d59ec",
   "metadata": {},
   "source": [
    "Because the values can only be 0 or 1, the predicted probability of an\n",
    "`appetite_dummy` of 0 must be 1 minus the (predicted probability of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_of_first_value = 1 - pp_of_1[0]\n",
    "pp_of_first_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270cfc1",
   "metadata": {},
   "source": [
    "We can therefore calculate the predicted probability of each 0 / 1 label in the\n",
    "data frame like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability of 0.\n",
    "pp_of_0 = 1 - pp_of_1\n",
    "# The next line sets all the label == 1 values correctly, but\n",
    "# the label == 0 values are now incorrect.\n",
    "pp_of_label = pp_of_1.copy()\n",
    "# Set the label == 0 values correctly.\n",
    "poor_appetite = appetite_d == 0\n",
    "pp_of_label[poor_appetite] = pp_of_0[poor_appetite]\n",
    "# Put this into the data frame for display\n",
    "hgb_predicted['pp_of_label'] = pp_of_label\n",
    "hgb_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcf17a",
   "metadata": {},
   "source": [
    "There's a fancy short-cut to the several lines above, that relies on the fact\n",
    "that multiplying by 0 gives 0.  We can form the `pp_of_label` column by first\n",
    "multiplying the 0 / 1 label column values by the values in `pp_of_1`.  This\n",
    "correctly sets the `pp_of_label` values for labels of 1, and leaves the\n",
    "remainder as 0. Then we reverse the 0 / 1 labels by subtracting from 1, and\n",
    "multiply by `pp_of_0` to set the 0 values to their correct values.  Adding\n",
    "these two results gives us the correct set of values for both 1 and 0 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ee0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set label == 1 values to predicted p, others to 0\n",
    "pos_ps = appetite_d * pp_of_1\n",
    "# Set label == 0 values to 1-predicted p, others to 0\n",
    "neg_ps = (1 - appetite_d) * pp_of_0\n",
    "# Combine by adding.\n",
    "final = pos_ps + neg_ps\n",
    "# This gives the same result as the code cell above:\n",
    "hgb_predicted['pp_of_label_fancy'] = final\n",
    "hgb_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b50678",
   "metadata": {},
   "source": [
    "We can do the whole cell above in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fe43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact version of pp of label calculation.\n",
    "final_again = appetite_d * pp_of_1 + (1 - appetite_d) * (1 - pp_of_1)\n",
    "hgb_predicted['pp_again'] = final_again\n",
    "hgb_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a54902",
   "metadata": {},
   "source": [
    "When the `pp_of_label` values are near 1, this means the result was close to\n",
    "the prediction.  When they are near 0, it means the result was unlike the\n",
    "prediction.\n",
    "\n",
    "Now we know the probabilities of each actual 0 and 1 label, we can get a\n",
    "measure of how *likely* the *combination* of all these labels are, given these\n",
    "predictions. We do this by *multiplying* all the probabilities in\n",
    "`pp_of_label`.  When the probabilities in `pp_of_label` are all fairly near 1,\n",
    "multiplying them will give a number that is not very small.  When some or many\n",
    "of the probabilities are close to 0, the multiplication will generate a very\n",
    "small number.\n",
    "\n",
    "The result of this multiplication is called the *likelihood* of this set of\n",
    "labels, given the predictions.\n",
    "\n",
    "If the predictions are closer to the actual values, the likelihood will be\n",
    "larger, and closer to 1.  When the predictions are not close, the likelihood\n",
    "will be low, and closer to 0.\n",
    "\n",
    "Here is the likelihood for our intercept of -7 and slope of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58cca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.prod multiplies each number to give the product of all the numbers.\n",
    "likelihood = np.prod(pp_of_label)\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7c897",
   "metadata": {},
   "source": [
    "This is a very small number.  Likelihoods are often close to 0 with a\n",
    "reasonable number of points, and somewhat inexact prediction, because there\n",
    "tend to be a reasonable number of small p values in `pp_of_labels`.  The\n",
    "question is - do other values for the intercept and slope give smaller or\n",
    "larger values for the likelihood?\n",
    "\n",
    "We can put this *likelihood* scoring into our cost function, instead of using\n",
    "scoring with the sum of squares.\n",
    "\n",
    "One wrinkle is that we want our cost function value to be *lower* when the\n",
    "line is a good predictor, but the likelihood is *higher* when the line is a\n",
    "good predictor. We solve this simply by sticking a minus on the likelihood\n",
    "before we return from the cost function. This makes `minimize` find the\n",
    "parameters giving the *minimum* of the *negative likelihood*, and therefore,\n",
    "the *maximum likelihood* (ML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ml_logit_cost(intercept_and_slope, x, y):\n",
    "    \"\"\" Simple version of cost function for maximum likelihood\n",
    "    \"\"\"\n",
    "    intercept, slope = intercept_and_slope\n",
    "    # Make predictions for sigmoid.\n",
    "    predicted_log_odds = intercept + slope * x\n",
    "    pp_of_1 = inv_logit(predicted_log_odds)\n",
    "    # Calculate predicted probabilities of actual labels.\n",
    "    pp_of_labels = y * pp_of_1 + (1 - y) * (1 - pp_of_1)\n",
    "    likelihood = np.prod(pp_of_labels)\n",
    "    # Ask minimize to find maximum by adding minus sign.\n",
    "    return -likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a5559",
   "metadata": {},
   "source": [
    "We can try that cost function, starting at our familiar intercept of -7 and\n",
    "slope of 0.8.\n",
    "\n",
    "Before we do, there is one extra wrinkle, that we will solve in another way\n",
    "further down the notebook.  At the moment the likelihood values we are\n",
    "returning are very small - in the order of $10^{-13}$.  We expect scores for\n",
    "different plausible lines to differ from each other by an even smaller number.\n",
    "By default, `minimize` will treat these very small differences as\n",
    "insignificant.  We can tell `minimize` to pay attention to these tiny\n",
    "differences by passing a very small value to the `tol` parameter (`tol` for\n",
    "*tolerance*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b13aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_ML = minimize(simple_ml_logit_cost,  # Cost function\n",
    "                 [-7, 0.8],  # Guessed intercept and slope\n",
    "                 args=(hemoglobin, appetite_d),  # x and y values\n",
    "                 tol=1e-16)  # Attend to tiny changes in cost function values.\n",
    "# Show the result.\n",
    "mr_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3be9f",
   "metadata": {},
   "source": [
    "`minimize` complains that it was not happy with the numerical accuracy of its\n",
    "answer, and we will address that soon.  For now, let us look at the predictions\n",
    "to see if they are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_ML, slope_ML = mr_ML.x\n",
    "predicted_ML = inv_logit(inter_ML + slope_ML * hemoglobin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a3517",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_hgb_app()\n",
    "plt.scatter(hemoglobin, predicted_ML, c='gold', label='ML prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29263f60",
   "metadata": {},
   "source": [
    "## A computation trick for the likelihood\n",
    "\n",
    "As you have seen, the likelihood values can be, and often are, very small, and\n",
    "close to zero.\n",
    "\n",
    "You may also know that standard numerical calculations on computers are not\n",
    "completely precise; the calculations are only accurate to around 16 decimal\n",
    "places.  This is because of the way computers store floating point numbers.\n",
    "You can find more detail in [this page on floating point\n",
    "numbers](http://matthew-brett.github.io/teaching/floating_point.html).\n",
    "\n",
    "The combination of small likelihood values, and limited calculation precision,\n",
    "can be a problem for the simple ML logistic cost function you see above.  As a\n",
    "result, practical implementations of logistic regression use an extra trick to\n",
    "improve the calculation accuracy of the likelihoods.\n",
    "\n",
    "This trick also involves logarithms.\n",
    "\n",
    "Here we show a very important property of the logarithm transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c6427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Multiplying two numbers', 11 * 15)\n",
    "print('Take logs, add logs, unlog', np.exp(np.log(11) + np.log(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add1d1b",
   "metadata": {},
   "source": [
    "What you see here is that we get (almost) the same answer if we:\n",
    "\n",
    "* Multiply two numbers OR\n",
    "* If we take the logs of the two numbers, *add them*, then reverse the log\n",
    "  operation, in this case with `np.exp`.\n",
    "\n",
    "We get *almost* the same number because of the limitations of the precision of\n",
    "the calculations.  For our cases, we do not need to worry about these tiny\n",
    "differences.\n",
    "\n",
    "The log-add-unlog trick means that we can replace multiplication by addition,\n",
    "if we take the logs of the values.\n",
    "\n",
    "Here we do the same trick on an array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_numbers = np.array([11, 15, 1, 0.3])\n",
    "print('Product of the array', np.prod(some_numbers))\n",
    "print('Log-add-unlog on the array', np.exp(np.sum(np.log(some_numbers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361c75b",
   "metadata": {},
   "source": [
    "We can use this same trick to calculate our likelihood by adding logs instead\n",
    "of multiplying the probabilities directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Likelihood with product of array', np.prod(pp_of_label))\n",
    "# The log-add-unlog version.\n",
    "logs_of_pp = np.log(pp_of_label)\n",
    "log_likelihood = np.sum(logs_of_pp)\n",
    "print('Likelihood with log-add-unlog', np.exp(log_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c766ee6",
   "metadata": {},
   "source": [
    "This doesn't seem to have solved all our problem, because we still end up with\n",
    "the type of tiny number that confuses `minimize`.\n",
    "\n",
    "The next stage of the solution is to realize that the `minimize` does not need\n",
    "the *actual likelihood*, it needs some number that goes *up or down in exactly\n",
    "the same way as likelihood*.  If the cost function can return some number that\n",
    "goes up when likelihood goes up, and goes down when likelihood goes down,\n",
    "`minimize` will still find the same parameters to minimize this cost function.\n",
    "\n",
    "Specifically we want the value that comes back from the cost function to vary\n",
    "[monotonically](https://en.wikipedia.org/wiki/Monotonic_function) with respect\n",
    "to the likelihood.  Consider any two pairs of (intercept, slope). Call the\n",
    "first pair `p1` and the second pair `p2`.  Say `p1` gives a likelihood value of\n",
    "`L1` and `p2` gives a likelihood value of `L2`. Now say we've got another way\n",
    "of calculating a value for `p1` and `p2`, and the corresponding values that\n",
    "come back from the new calculation are `N1` and `N2`.  Remember `miminize` is\n",
    "searching for the (intercept, slope) pair that gives the lowest value from the\n",
    "cost function.  So, we can swap the new calculation into our cost function, and\n",
    "get the same parameters back from `miminize`, as long as it is always true\n",
    "that:\n",
    "\n",
    "* when `L1 > L2` then `N1 > N2`;\n",
    "* when `L1 < L2` then `N1 < N2`; and\n",
    "* when `L1 == L2`, `N1 == N2`.\n",
    "\n",
    "In that case we call our new calculation generating `N1` and `N2` *monotonic*\n",
    "with respect to the original calculation generating `L1` and `L2`.  In our case\n",
    "this original calculation is likelihood. The new monotonic calculation is the\n",
    "*log likelihood*.\n",
    "\n",
    "In the plot below, you can see it is true that when the likelihood goes up,\n",
    "then the log of the likelihood will also go up, and vice versa, so the log the\n",
    "likelihood *is* monotonic with respect to the likelihood, and we can use it\n",
    "instead of the likelihood, in our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_values = np.linspace(0, 1, 1000)\n",
    "plt.plot(likelihood_values, np.log(likelihood_values))\n",
    "plt.xlabel('Likelihood')\n",
    "plt.ylabel('Log likelihood');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a58307",
   "metadata": {},
   "source": [
    "This means that our cost function does not have to do the last nasty unlog step\n",
    "above, that generates the tiny value for likelihood.  We can just return the\n",
    "(minus of the) log of the likelihood.  The log of the likelihood turns out to\n",
    "be a manageable negative number, even when the resulting likelihood is tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd147c80",
   "metadata": {},
   "source": [
    "This trick gives us a much more tractable number to return from the cost\n",
    "function, because the log likelihood is much less affected by errors from lack\n",
    "of precision in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d501bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mll_logit_cost(intercept_and_slope, x, y):\n",
    "    \"\"\" Cost function for maximum log likelihood\n",
    "\n",
    "    Return minus of the log of the likelihood.\n",
    "    \"\"\"\n",
    "    intercept, slope = intercept_and_slope\n",
    "    # Make predictions for sigmoid.\n",
    "    predicted_log_odds = intercept + slope * x\n",
    "    pp_of_1 = inv_logit(predicted_log_odds)\n",
    "    # Calculate predicted probabilities of actual labels.\n",
    "    pp_of_labels = y * pp_of_1 + (1 - y) * (1 - pp_of_1)\n",
    "    # Use logs to calculate log of the likelihood\n",
    "    log_likelihood = np.sum(np.log(pp_of_labels))\n",
    "    # Ask minimize to find maximum by adding minus sign.\n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3feb1c",
   "metadata": {},
   "source": [
    "Use the new cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c441831",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_MLL = minimize(mll_logit_cost,  # Cost function\n",
    "                  [-7, 0.8],  # Guessed intercept and slope\n",
    "                  args=(hemoglobin, appetite_d),  # x and y values\n",
    "                  )\n",
    "# Show the result.\n",
    "mr_MLL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa32525",
   "metadata": {},
   "source": [
    "Notice that we did not have to tell `minimize` to use a very small value for\n",
    "the tolerance this time.\n",
    "\n",
    "The values that come back are very similar to our previous, more fragile\n",
    "version that used the likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_MLL, slope_MLL = mr_MLL.x\n",
    "predicted_MLL = inv_logit(inter_MLL + slope_MLL * hemoglobin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111b765",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_hgb_app()\n",
    "plt.scatter(hemoglobin, predicted_ML, c='gold', label='MLL prediction')\n",
    "plt.title('Maximum log likelihood cost function')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db234b0c",
   "metadata": {},
   "source": [
    "You have just seen the standard calculations behind most packages that\n",
    "implement logistic regression.\n",
    "\n",
    "To show this is standard, let us do the same regression in Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c12881",
   "metadata": {},
   "source": [
    "## Logistic Regression with Statsmodes\n",
    "\n",
    "As with linear regression, we can easily perform logistic regression using\n",
    "Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the formula interface for Statsmodels\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9be4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "log_reg_mod = smf.logit('appetite_dummy ~ Hemoglobin', data=hgb_app)\n",
    "# Fit it.\n",
    "fitted_log_reg_mod = log_reg_mod.fit()\n",
    "fitted_log_reg_mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9bd31b",
   "metadata": {},
   "source": [
    "Notice that Statsmodels lists the \"Model\" as \"Logit\" and the \"Method\" as \"MLE\"\n",
    "— Maximum Likelihood Estimation.\n",
    "\n",
    "Look at the table above under 'coef'. Compare the logistic regression intercept\n",
    "and slope that Statsmodels found to the ones we got from `minimize` and the\n",
    "maximum log likelihood (MLL) cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept from minimize =', inter_MLL)\n",
    "print('Slope from minimize =', slope_MLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe695a1",
   "metadata": {},
   "source": [
    "Finally, we can use the `predict` method of Statsmodels to generate predicted\n",
    "probabilities from the logistic regression model we have just fitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_predictions = fitted_log_reg_mod.predict(hgb_app['Hemoglobin'])\n",
    "sm_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fba33a",
   "metadata": {},
   "source": [
    "Let us plot the predicted probabilities of having \"good\" appetite, from\n",
    "Statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f9bca",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_hgb_app()\n",
    "plt.scatter(hemoglobin, sm_predictions,\n",
    "            label = 'Statsmodels p', color = 'cyan')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278f9ef",
   "metadata": {},
   "source": [
    "We can see graphically that these predictions look identical to the ones we\n",
    "obtained from minimize.\n",
    "\n",
    "Let us see what the largest absolute difference between the predictions from\n",
    "the two methods is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(predicted_MLL - sm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119b754",
   "metadata": {},
   "source": [
    "That is very close to 0. The models are making almost identical predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a180d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial has shown you how to do binary logistic regression with one numerical predictor variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34087321",
   "metadata": {},
   "source": [
    "## Exercises for reflection\n",
    "\n",
    "You might want to read the [logarithm refresher](logarithms_refreshed) page for\n",
    "these questions.\n",
    "\n",
    "* Consider the expression `np.exp(y)` in the `inv_logit` expression above.\n",
    "  This means \"raise `np.e` to the power of the values in `y`\", and could\n",
    "  equally well be written as `np.e ** y` (it would give exactly the same\n",
    "  result).  Now consider changing `np.exp(y)` to `10 ** y`.  Now we are working\n",
    "  in base 10 instead of base `np.e`.  What would happen to:\n",
    "\n",
    "  * The best-fit p values predictions returned via `minimize`?\n",
    "  * The slope and intercept of the best-fit log-odds straight line?\n",
    "\n",
    "  Reflect, using the reasoning here, then try it and see.\n",
    "\n",
    "* Consider the expression `np.sum(np.log(pp_of_labels))` in the\n",
    "  `mll_logit_cost` cost function.  This is using `np.log`, which is log to the\n",
    "  base `np.e`.  Consider what would happen if you change this expression to\n",
    "  `np.sum(np.log10(pp_of_labels))` (log to the base 10).  What would happen to:\n",
    "\n",
    "  * The best-fit p values predictions returned via `minimize`?\n",
    "  * The slope and intercept of the best-fit log-odds straight line?\n",
    "\n",
    "  Reflect, using the reasoning here, then try it and see."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-language_info",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
